{"cells":[{"cell_type":"markdown","metadata":{"id":"kSEH5Avo3qT4"},"source":["**To get acces to webcam in the colab notebook**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wX0MK97dzkqi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652339333281,"user_tz":-120,"elapsed":18741,"user":{"displayName":"Tanguy Lewko","userId":"14686919135043018325"}},"outputId":"bb3fa964-c3ac-4dd1-db5d-7617bd0307f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":[" from google.colab import drive\n"," drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wtHf1fvwfEU"},"outputs":[],"source":["# import dependencies\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pT0O3cV7wh9A"},"outputs":[],"source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","          bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ju604mJZxQQj"},"outputs":[],"source":["# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 640);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 640; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_owVKXm3qQs_"},"outputs":[],"source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","\n","def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","  data = eval_js('takePhoto({})'.format(quality))\n","  binary = b64decode(data.split(',')[1])\n","  with open(filename, 'wb') as f:\n","    f.write(binary)\n","  return filename"]},{"cell_type":"code","source":["import torch\n","model = torch.hub.load('/content/gdrive/MyDrive/DLAVproj/yolov5', 'custom', path='/content/gdrive/MyDrive/DLAVproj/best_weight.pt', source='local')  # local repo\n","!pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n","model2 = torch.hub.load('ultralytics/yolov5', 'yolov5s')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6634323895934535bf21c0569c70ef83","d155559b53b746acb85af407c502e270","b5fb0036a64a4ddea4f51be4ae10de44","477118406fe94b759460ef4f7468ac66","91d982a5876645f08663da33b29fd11b","3dde254f48704be59df7b629ec1b908c","5365b1c79dff47439f54cc8c02ba5228","2cef94c1097b4e55b5b622ab4f405670","3615d97cd98e4a67978acf6986d9fb04","282d00f15eb94f638871c13edbd7eae5","6bb32eb9c782422e9bc8ed94e28d9e91"]},"id":"0rEVJcRSPME1","executionInfo":{"status":"ok","timestamp":1652339379193,"user_tz":-120,"elapsed":38663,"user":{"displayName":"Tanguy Lewko","userId":"14686919135043018325"}},"outputId":"fa3c202b-6caa-4ecd-861f-740140352b54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n","Collecting PyYAML>=5.3.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","Installing collected packages: PyYAML\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-6.0\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /content/gdrive/MyDrive/Checkpoint 1 V3/yolov5/requirements.txt\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","YOLOv5 üöÄ 2022-4-25 torch 1.11.0+cu113 CUDA:0 (Tesla K80, 11441MiB)\n","\n","Fusing layers... \n","Model summary: 213 layers, 7012822 parameters, 0 gradients\n","Adding AutoShape... \n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 4)) (3.2.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 5)) (1.21.6)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 6)) (4.1.2.30)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 7)) (7.1.2)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 8)) (6.0)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 9)) (2.23.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 10)) (1.4.1)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 11)) (1.11.0+cu113)\n","Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 12)) (0.12.0+cu113)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 13)) (4.64.0)\n","Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (2.8.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 20)) (1.3.5)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 21)) (0.11.2)\n","Collecting thop\n","  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 4)) (1.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 4)) (3.0.8)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 4)) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 4)) (0.11.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 9)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 9)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 9)) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 9)) (1.24.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 11)) (4.2.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (1.44.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (0.6.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (57.4.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (3.3.6)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (3.17.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (0.37.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (1.35.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (1.0.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (1.0.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 20)) (2022.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (3.2.0)\n","Installing collected packages: thop\n","Successfully installed thop-0.0.31.post2005241907\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n","\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (6.0)\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /content/gdrive/MyDrive/Checkpoint 1 V3/yolov5/requirements.txt\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","YOLOv5 üöÄ 2022-4-25 torch 1.11.0+cu113 CUDA:0 (Tesla K80, 11441MiB)\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/14.1M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6634323895934535bf21c0569c70ef83"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","Fusing layers... \n","YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n","Adding AutoShape... \n"]}]},{"cell_type":"code","source":["# NMS confidence threshold\n","model.max_det = 1  # maximum number of detections per image\n","model.conf = 0.7\n","model2.conf = 0.7\n","model2.classes = [0]"],"metadata":{"id":"coZW9zj3RhTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install deep-sort-realtime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6iWTmaprRht8","executionInfo":{"status":"ok","timestamp":1652339385059,"user_tz":-120,"elapsed":5910,"user":{"displayName":"Tanguy Lewko","userId":"14686919135043018325"}},"outputId":"4293f756-3821-43d1-cd88-5f275b9112be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting deep-sort-realtime\n","  Downloading deep_sort_realtime-1.2-py3-none-any.whl (8.4 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.4 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from deep-sort-realtime) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deep-sort-realtime) (1.21.6)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from deep-sort-realtime) (4.1.2.30)\n","Installing collected packages: deep-sort-realtime\n","Successfully installed deep-sort-realtime-1.2\n"]}]},{"cell_type":"code","source":["from deep_sort_realtime.deepsort_tracker import DeepSort"],"metadata":{"id":"Cxq72-HjR3hV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","import pdb\n","import json\n","import time\n","import PIL\n","\n","tracker = DeepSort(max_age=30, nn_budget=70, override_track_class=None)\n","\n","# start streaming video from webcam\n","video_stream()\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox_f = ''\n","xmin_best = 0\n","ymin_best = 0\n","xmax_best = 0\n","ymax_best = 0\n","previous = 0\n","best_intersection = 0\n","# ok = False\n","init_detection = False\n","first_track = False\n","\n","\n","while True:\n","    js_reply = video_frame(label_html, bbox_f)\n","    if not js_reply:\n","        break\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img_model = PIL.Image.fromarray(img_rgb, \"RGB\")\n","\n","    if init_detection == False :\n","      # Obtaining a bgr image\n","      # Inferences\n","      results = model(img_model)\n","      results2 = model2(img_model)\n","      results = results.pandas().xyxy[0].to_json(orient=\"records\")  # JSON img1 predictions\n","      results2 = results2.pandas().xyxy[0].to_json(orient=\"records\")  # JSON img1 predictions\n","      results = json.loads(results) # get a dictionnary from the string\n","      results2 = json.loads(results2) # get a dictionnary from the string\n","      bbox_array = np.zeros([640,640,4], dtype=np.uint8)\n","\n","      # Initializing the tracking\n","      detection = False\n","      for result in results:\n","          detection = True\n","          xmin = int(result.get('xmin'))\n","          ymin = int(result.get('ymin'))\n","          xmax = int(result.get('xmax'))\n","          ymax = int(result.get('ymax'))\n","          confidence = str(result.get('confidence'))\n","          bbox_array = cv2.rectangle(bbox_array,(xmin,ymin),(xmax,ymax),(255,0,0),2)\n","          bbox_array = cv2.putText(bbox_array, \"[{conf}]\".format(conf = confidence),\n","                          (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n","                          (255,0,0), 2)\n","          \n","      if detection == True :\n","        for idx, result2 in enumerate(results2):\n","            if result2.get('name') == 'person':\n","              xmin2 = int(result2.get('xmin'))\n","              ymin2 = int(result2.get('ymin'))\n","              xmax2 = int(result2.get('xmax'))\n","              ymax2 = int(result2.get('ymax'))\n","              confidence2 = str(result2.get('confidence'))\n","              class2 = str(result2.get('class'))\n","              x_left = max(xmin, xmin2)\n","              y_top = max(ymin, ymin2)\n","              x_right = min(xmax, xmax2)\n","              y_bottom = min(ymax, ymax2)\n","              intersection = (x_right - x_left)*(y_bottom - y_top)\n","              if intersection > previous:\n","                best_intersection = intersection\n","                xmin_best = xmin2\n","                ymin_best = ymin2\n","                xmax_best = xmax2\n","                ymax_best = ymax2\n","                best_confidence = confidence2\n","                best_class = class2\n","              previous = intersection\n","              intersection = 0\n","        if best_intersection != 0 :\n","          bbox_array = cv2.rectangle(bbox_array,(xmin_best,ymin_best),(xmax_best,ymax_best),(0,0,255),2)\n","          init_detection = True\n","          bbox = [xmin_best, ymin_best, xmax_best - xmin_best, ymax_best - ymin_best]\n","          detections = [(bbox, confidence, best_class)]\n","\n","    if init_detection == True :\n","        ##\n","        if first_track == True :\n","          results2 = model2(img_model)\n","          results2 = results2.pandas().xyxy[0].to_json(orient=\"records\")  # JSON img1 predictions\n","          results2 = json.loads(results2) # get a dictionnary from the string\n","          detections = []\n","          for idx, result2 in enumerate(results2):\n","              if result2.get('name') == 'person':\n","                xmin2 = int(result2.get('xmin'))\n","                ymin2 = int(result2.get('ymin'))\n","                xmax2 = int(result2.get('xmax'))\n","                ymax2 = int(result2.get('ymax'))\n","                confidence2 = float(result2.get('confidence'))\n","                class2 = str(result2.get('class'))\n","                detections.append(([xmin2, ymin2, xmax2-xmin2, ymax2-ymin2], confidence2, class2))\n","\n","        bbox_array = np.zeros([640,640,4], dtype=np.uint8)\n","        tracks = tracker.update_tracks(detections, frame=img) # bbs expected to be a list of detections, each in tuples of ( [left,top,w,h], confidence, detection_class )\n","        for track in tracks:\n","          track_id = track.track_id\n","          if first_track == False:\n","            target_id = track_id\n","          ltrb = track.to_ltrb()  \n","          if track_id == target_id :   \n","            bbox_array = cv2.rectangle(bbox_array, (int(ltrb[0]),int(ltrb[1])),(int(ltrb[2]),int(ltrb[3])),(0,0,255),2,1)\n","          else:\n","            bbox_array = cv2.rectangle(bbox_array, (int(ltrb[0]),int(ltrb[1])),(int(ltrb[2]),int(ltrb[3])),(0,255,0),2,1)\n","          bbox_array = cv2.putText(bbox_array, \"[{id}]\".format(id = track_id),\n","                    (int(ltrb[0]), int(ltrb[1]) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n","                    (255,0,0), 2)\n","        first_track = True\n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    \n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","\n","    # update bbox so next frame gets new overlay5\n","    bbox_f = bbox_bytes"],"metadata":{"id":"P3dgZc0M3Tfn","colab":{"base_uri":"https://localhost:8080/","height":260},"executionInfo":{"status":"ok","timestamp":1652341707486,"user_tz":-120,"elapsed":38506,"user":{"displayName":"Tanguy Lewko","userId":"14686919135043018325"}},"outputId":"261bcf3a-383c-4c57-b851-31179ff7460d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["MobileNetV2 Embedder for Deep Sort initialised\n","- gpu enabled: True\n","- half precision: True\n","- max batch size: 16\n","- expects BGR: True\n","DeepSort Tracker initialised\n","- max age: 30\n","- appearance threshold: 0.2\n","- nms threshold: OFF\n","- max num of appearance features: 70\n","- overriding track class : No\n","- today given : No\n","- in-build embedder : Yes\n","- polygon detections : No\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 640);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 640; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    "]},"metadata":{}}]},{"cell_type":"code","source":[""],"metadata":{"id":"wzcx1v1M_S_K"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Tracking.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6634323895934535bf21c0569c70ef83":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d155559b53b746acb85af407c502e270","IPY_MODEL_b5fb0036a64a4ddea4f51be4ae10de44","IPY_MODEL_477118406fe94b759460ef4f7468ac66"],"layout":"IPY_MODEL_91d982a5876645f08663da33b29fd11b"}},"d155559b53b746acb85af407c502e270":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dde254f48704be59df7b629ec1b908c","placeholder":"‚Äã","style":"IPY_MODEL_5365b1c79dff47439f54cc8c02ba5228","value":"100%"}},"b5fb0036a64a4ddea4f51be4ae10de44":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cef94c1097b4e55b5b622ab4f405670","max":14808437,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3615d97cd98e4a67978acf6986d9fb04","value":14808437}},"477118406fe94b759460ef4f7468ac66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_282d00f15eb94f638871c13edbd7eae5","placeholder":"‚Äã","style":"IPY_MODEL_6bb32eb9c782422e9bc8ed94e28d9e91","value":" 14.1M/14.1M [00:00&lt;00:00, 24.3MB/s]"}},"91d982a5876645f08663da33b29fd11b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dde254f48704be59df7b629ec1b908c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5365b1c79dff47439f54cc8c02ba5228":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cef94c1097b4e55b5b622ab4f405670":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3615d97cd98e4a67978acf6986d9fb04":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"282d00f15eb94f638871c13edbd7eae5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bb32eb9c782422e9bc8ed94e28d9e91":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}